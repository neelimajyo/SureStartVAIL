{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Building Autoencoders in Keras",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gevi1wFTF3Be"
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "encoding_dim = 32  \n",
        "input_img = keras.Input(shape=(784,))\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "encoded = layers.Dense(encoding_dim, activation='relu')(input_img)\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
        "autoencoder = keras.Model(input_img, decoded)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DGtkkNxG36U"
      },
      "source": [
        "# Separate encoder/decoder model\n",
        "encoder = keras.Model(input_img, encoded)\n",
        "encoded_input = keras.Input(shape=(encoding_dim,))\n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD8uzxIlG93K"
      },
      "source": [
        "#Configuring model\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0TG2pv4HOFc",
        "outputId": "8028eb4f-517e-4f84-9c6a-8f264f7bf515"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "(x_train, _), (x_test, _) = mnist.load_data()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unEHmgk3HOft",
        "outputId": "743523de-5d40-47ab-e9d9-552dd1c7349d"
      },
      "source": [
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH68mam_HQrF",
        "outputId": "3dd5fa30-c5c6-48a7-fe0a-79960c7f386e"
      },
      "source": [
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=50,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.1445 - val_loss: 0.1339\n",
            "Epoch 2/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.1288 - val_loss: 0.1215\n",
            "Epoch 3/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.1187 - val_loss: 0.1133\n",
            "Epoch 4/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.1117 - val_loss: 0.1076\n",
            "Epoch 5/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.1067 - val_loss: 0.1032\n",
            "Epoch 6/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.1027 - val_loss: 0.0999\n",
            "Epoch 7/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0998 - val_loss: 0.0974\n",
            "Epoch 8/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0977 - val_loss: 0.0955\n",
            "Epoch 9/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0963 - val_loss: 0.0944\n",
            "Epoch 10/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0953 - val_loss: 0.0937\n",
            "Epoch 11/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0948 - val_loss: 0.0932\n",
            "Epoch 12/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0944 - val_loss: 0.0930\n",
            "Epoch 13/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0941 - val_loss: 0.0926\n",
            "Epoch 14/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0939 - val_loss: 0.0924\n",
            "Epoch 15/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0937 - val_loss: 0.0923\n",
            "Epoch 16/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0936 - val_loss: 0.0923\n",
            "Epoch 17/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0934 - val_loss: 0.0921\n",
            "Epoch 18/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0934 - val_loss: 0.0920\n",
            "Epoch 19/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0933 - val_loss: 0.0921\n",
            "Epoch 20/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0932 - val_loss: 0.0919\n",
            "Epoch 21/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0931 - val_loss: 0.0919\n",
            "Epoch 22/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0931 - val_loss: 0.0918\n",
            "Epoch 23/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0930 - val_loss: 0.0918\n",
            "Epoch 24/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0930 - val_loss: 0.0918\n",
            "Epoch 25/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0930 - val_loss: 0.0917\n",
            "Epoch 26/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0929 - val_loss: 0.0917\n",
            "Epoch 27/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0929 - val_loss: 0.0917\n",
            "Epoch 28/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0929 - val_loss: 0.0916\n",
            "Epoch 29/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0928 - val_loss: 0.0917\n",
            "Epoch 30/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0928 - val_loss: 0.0917\n",
            "Epoch 31/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0928 - val_loss: 0.0916\n",
            "Epoch 32/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0928 - val_loss: 0.0916\n",
            "Epoch 33/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0927 - val_loss: 0.0916\n",
            "Epoch 34/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0927 - val_loss: 0.0916\n",
            "Epoch 35/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0927 - val_loss: 0.0916\n",
            "Epoch 36/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0927 - val_loss: 0.0915\n",
            "Epoch 37/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0927 - val_loss: 0.0916\n",
            "Epoch 38/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0927 - val_loss: 0.0916\n",
            "Epoch 39/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0926 - val_loss: 0.0915\n",
            "Epoch 40/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0926 - val_loss: 0.0915\n",
            "Epoch 41/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0926 - val_loss: 0.0915\n",
            "Epoch 42/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0926 - val_loss: 0.0915\n",
            "Epoch 43/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0926 - val_loss: 0.0915\n",
            "Epoch 44/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0926 - val_loss: 0.0915\n",
            "Epoch 45/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0926 - val_loss: 0.0914\n",
            "Epoch 46/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0926 - val_loss: 0.0914\n",
            "Epoch 47/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0926 - val_loss: 0.0915\n",
            "Epoch 48/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0925 - val_loss: 0.0914\n",
            "Epoch 49/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0925 - val_loss: 0.0914\n",
            "Epoch 50/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0925 - val_loss: 0.0915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8d87438a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh27nz-oHYRg"
      },
      "source": [
        "# Now going to upsample\n",
        "encoded_imgs = encoder.predict(x_test)\n",
        "decoded_imgs = decoder.predict(encoded_imgs)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "qbmOj-XYHqyu",
        "outputId": "521f8fd5-45a7-4d4f-c055-be6ed98a919c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#Number of digits to display\n",
        "n = 5\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03xZkwy4IFQV"
      },
      "source": [
        "from keras import regularizers\n",
        "\n",
        "encoding_dim = 32\n",
        "\n",
        "input_img = keras.Input(shape=(784,))\n",
        "# Add a Dense layer with a L1 activity regularizer <<--- \n",
        "encoded = layers.Dense(encoding_dim, activation='relu',\n",
        "                activity_regularizer=regularizers.l1(10e-5))(input_img)\n",
        "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "autoencoder = keras.Model(input_img, decoded)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4SKJtHVIUkt"
      },
      "source": [
        "# Building multiple layers \n",
        "input_img = keras.Input(shape=(784,))\n",
        "encoded = layers.Dense(128, activation='relu')(input_img)\n",
        "encoded = layers.Dense(64, activation='relu')(encoded)\n",
        "encoded = layers.Dense(32, activation='relu')(encoded)\n",
        "\n",
        "decoded = layers.Dense(64, activation='relu')(encoded)\n",
        "decoded = layers.Dense(128, activation='relu')(decoded)\n",
        "decoded = layers.Dense(784, activation='sigmoid')(decoded)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXRzohlDIaG3",
        "outputId": "b4d1fd1c-443f-4ada-8ffc-ed42a0d21c63"
      },
      "source": [
        "autoencoder = keras.Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=100,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.3430 - val_loss: 0.1690\n",
            "Epoch 2/100\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1600 - val_loss: 0.1377\n",
            "Epoch 3/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1352 - val_loss: 0.1253\n",
            "Epoch 4/100\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1251 - val_loss: 0.1185\n",
            "Epoch 5/100\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1179 - val_loss: 0.1127\n",
            "Epoch 6/100\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1132 - val_loss: 0.1102\n",
            "Epoch 7/100\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1098 - val_loss: 0.1064\n",
            "Epoch 8/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1070 - val_loss: 0.1041\n",
            "Epoch 9/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1050 - val_loss: 0.1021\n",
            "Epoch 10/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1032 - val_loss: 0.1007\n",
            "Epoch 11/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1014 - val_loss: 0.0996\n",
            "Epoch 12/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1001 - val_loss: 0.0979\n",
            "Epoch 13/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0983 - val_loss: 0.0968\n",
            "Epoch 14/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0972 - val_loss: 0.0952\n",
            "Epoch 15/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0962 - val_loss: 0.0949\n",
            "Epoch 16/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0950 - val_loss: 0.0935\n",
            "Epoch 17/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0943 - val_loss: 0.0930\n",
            "Epoch 18/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0934 - val_loss: 0.0919\n",
            "Epoch 19/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0928 - val_loss: 0.0913\n",
            "Epoch 20/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0921 - val_loss: 0.0910\n",
            "Epoch 21/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0917 - val_loss: 0.0905\n",
            "Epoch 22/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0911 - val_loss: 0.0900\n",
            "Epoch 23/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0907 - val_loss: 0.0899\n",
            "Epoch 24/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0900 - val_loss: 0.0897\n",
            "Epoch 25/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0899 - val_loss: 0.0889\n",
            "Epoch 26/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0896 - val_loss: 0.0885\n",
            "Epoch 27/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0894 - val_loss: 0.0881\n",
            "Epoch 28/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0892 - val_loss: 0.0883\n",
            "Epoch 29/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0887 - val_loss: 0.0878\n",
            "Epoch 30/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0882 - val_loss: 0.0878\n",
            "Epoch 31/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0881 - val_loss: 0.0873\n",
            "Epoch 32/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0879 - val_loss: 0.0872\n",
            "Epoch 33/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0876 - val_loss: 0.0870\n",
            "Epoch 34/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0876 - val_loss: 0.0872\n",
            "Epoch 35/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0872 - val_loss: 0.0867\n",
            "Epoch 36/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0872 - val_loss: 0.0865\n",
            "Epoch 37/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0871 - val_loss: 0.0866\n",
            "Epoch 38/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0869 - val_loss: 0.0861\n",
            "Epoch 39/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0868 - val_loss: 0.0860\n",
            "Epoch 40/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0867 - val_loss: 0.0862\n",
            "Epoch 41/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0864 - val_loss: 0.0860\n",
            "Epoch 42/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0864 - val_loss: 0.0859\n",
            "Epoch 43/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0860 - val_loss: 0.0855\n",
            "Epoch 44/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0862 - val_loss: 0.0855\n",
            "Epoch 45/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0860 - val_loss: 0.0856\n",
            "Epoch 46/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0858 - val_loss: 0.0853\n",
            "Epoch 47/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0859 - val_loss: 0.0851\n",
            "Epoch 48/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0858 - val_loss: 0.0852\n",
            "Epoch 49/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0854 - val_loss: 0.0853\n",
            "Epoch 50/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0854 - val_loss: 0.0851\n",
            "Epoch 51/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0854 - val_loss: 0.0848\n",
            "Epoch 52/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0854 - val_loss: 0.0851\n",
            "Epoch 53/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0853 - val_loss: 0.0846\n",
            "Epoch 54/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0852 - val_loss: 0.0846\n",
            "Epoch 55/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0848 - val_loss: 0.0848\n",
            "Epoch 56/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0849 - val_loss: 0.0847\n",
            "Epoch 57/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0851 - val_loss: 0.0845\n",
            "Epoch 58/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0847 - val_loss: 0.0844\n",
            "Epoch 59/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0849 - val_loss: 0.0843\n",
            "Epoch 60/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0846 - val_loss: 0.0843\n",
            "Epoch 61/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0844 - val_loss: 0.0846\n",
            "Epoch 62/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0845 - val_loss: 0.0840\n",
            "Epoch 63/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0845 - val_loss: 0.0842\n",
            "Epoch 64/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0844 - val_loss: 0.0844\n",
            "Epoch 65/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0843 - val_loss: 0.0840\n",
            "Epoch 66/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0843 - val_loss: 0.0839\n",
            "Epoch 67/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0844 - val_loss: 0.0841\n",
            "Epoch 68/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0843 - val_loss: 0.0840\n",
            "Epoch 69/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0842 - val_loss: 0.0841\n",
            "Epoch 70/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0840 - val_loss: 0.0837\n",
            "Epoch 71/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0841 - val_loss: 0.0837\n",
            "Epoch 72/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0840 - val_loss: 0.0838\n",
            "Epoch 73/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0840 - val_loss: 0.0839\n",
            "Epoch 74/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0838 - val_loss: 0.0836\n",
            "Epoch 75/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0838 - val_loss: 0.0837\n",
            "Epoch 76/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0839 - val_loss: 0.0835\n",
            "Epoch 77/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0838 - val_loss: 0.0835\n",
            "Epoch 78/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0838 - val_loss: 0.0835\n",
            "Epoch 79/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0837 - val_loss: 0.0837\n",
            "Epoch 80/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0836 - val_loss: 0.0838\n",
            "Epoch 81/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0836 - val_loss: 0.0836\n",
            "Epoch 82/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0835 - val_loss: 0.0837\n",
            "Epoch 83/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0833 - val_loss: 0.0834\n",
            "Epoch 84/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0834 - val_loss: 0.0833\n",
            "Epoch 85/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0831 - val_loss: 0.0835\n",
            "Epoch 86/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0834 - val_loss: 0.0831\n",
            "Epoch 87/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0834 - val_loss: 0.0833\n",
            "Epoch 88/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0833 - val_loss: 0.0832\n",
            "Epoch 89/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0833 - val_loss: 0.0831\n",
            "Epoch 90/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0831 - val_loss: 0.0830\n",
            "Epoch 91/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0832 - val_loss: 0.0832\n",
            "Epoch 92/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0831 - val_loss: 0.0831\n",
            "Epoch 93/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0834 - val_loss: 0.0830\n",
            "Epoch 94/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0831 - val_loss: 0.0832\n",
            "Epoch 95/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0830 - val_loss: 0.0830\n",
            "Epoch 96/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0833 - val_loss: 0.0831\n",
            "Epoch 97/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0833 - val_loss: 0.0829\n",
            "Epoch 98/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0829 - val_loss: 0.0828\n",
            "Epoch 99/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0827 - val_loss: 0.0828\n",
            "Epoch 100/100\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0828 - val_loss: 0.0829\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8d84d0d850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sKiSMqpIg2p"
      },
      "source": [
        "#Building a CNN\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902
        },
        "id": "KAdoYxHKJuCp",
        "outputId": "e5eab1c7-1abd-49db-fc35-b7344fd261c8"
      },
      "source": [
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=50,\n",
        "                batch_size=128,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test),\n",
        "                ## Question about Tensorboard on GoogleCoLab"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-3b6b3754a0be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m-> 2941\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3356\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m   3357\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[0;32m-> 3358\u001b[0;31m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3279\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3280\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3281\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:754 train_step\n        y_pred = self(x, training=True)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:274 assert_input_compatibility\n        ', found shape=' + display_shape(x.shape))\n\n    ValueError: Input 0 is incompatible with layer model_6: expected shape=(None, 784), found shape=(None, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHIzKXRlJwjg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}